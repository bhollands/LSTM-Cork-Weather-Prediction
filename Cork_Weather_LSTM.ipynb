{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Cork Data\n",
    "\n",
    "I will use the data from the to predict 3 days into the future [MET Office](https://www.met.ie//climate/available-data/historical-data). This data is hourly and goes from \\[01-Jan-1992 $\\rightarrow$ 01-Feb-2022\\]. \n",
    "\n",
    "According to the key provided with the dataset the columns are:\n",
    "- **rain** | Precipitation Amount | mm\n",
    "- **temp** | Air Temperature | °C\n",
    "- **wetb** | Wet Bulb Air Temperature | °C\n",
    "- **dewpt** | Dew Point Air Temperature | °C                 \n",
    "- **vappr** |Vapour Pressure | hpa\n",
    "- **rhum** | Relative Humidity | %\n",
    "- **msl** | Mean Sea Level Pressure | hPa\n",
    "- **wdsp** | Mean Hourly Wind Speed | kt\n",
    "- **wddir** | Predominant Hourly wind Direction | kt\n",
    "- **ww** | Synop Code Present Weather\n",
    "- **w**| Synop Code Past Weather\n",
    "- **sun** | Sunshine duration | hours\n",
    "- **vis** | Visibility | m\n",
    "- **clht** | Cloud Ceiling Height | 100s feet\n",
    "- **clamt** | Cloud Amount | \n",
    "------------------------\n",
    "Using this data I will train a reccurent model that will be able to predict the next 48 hours into the future given the previous 72 hours weather. I will train 2 different types, one will just predict the tempurature as with the tutorial, next I will predict the tempurature, rainfall and the wind vector as in my opinion these 3 are the most important factor in gauging general weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras\n",
    "import seaborn as sns\n",
    "OUT_STEPS = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining my own plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom plotting function\n",
    "def plot_results(model, window,features, subplot = False):\n",
    "    pred_num = len(features)\n",
    "    plt.figure(figsize = (20,3))\n",
    "    label_width = OUT_STEPS\n",
    "    shift = OUT_STEPS\n",
    "    input_width = 72\n",
    "    batch_num = 1\n",
    "    total_window_size = input_width + shift\n",
    "\n",
    "    input_slice = slice(0, input_width)\n",
    "    input_indices = np.arange(total_window_size)[input_slice]\n",
    "\n",
    "\n",
    "    label_start = total_window_size - label_width\n",
    "    labels_slice = slice(label_start, None)\n",
    "    label_indices = np.arange(total_window_size)[labels_slice]\n",
    "    \n",
    "    for i, l in window.test.take(1):\n",
    "        inputs = i\n",
    "        labels = l\n",
    "        break\n",
    "        \n",
    "    column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "    predictions = model(inputs)\n",
    "    print(predictions.shape)\n",
    "    cols = ['red','orange','blue','green','yellow']\n",
    "\n",
    "    for i in range(pred_num):\n",
    "        if subplot:\n",
    "            plt.figure(figsize = (20,10))\n",
    "            plt.subplot(pred_num,1,i+1)\n",
    "            plt.title(features[i])\n",
    "            \n",
    "        plot_col_index = column_indices[features[i]]\n",
    "        plt.plot(input_indices, inputs[batch_num, :, plot_col_index],label='Input', marker='.', zorder=-10,color = cols[i])\n",
    "        plt.scatter(label_indices, labels[batch_num,:,i], marker ='o',label = 'True' ,color = cols[i])\n",
    "        #plt.scatter(label_indices, labels[batch_num,:,i], marker ='o',label = 'Prediction' ,color = cols[i])\n",
    "        plt.scatter(label_indices, predictions[batch_num,:,i], marker ='x', color = 'green', label = 'prediction')\n",
    "        plt.legend()\n",
    "        plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "I first need to read in the data using pandas and check that all the data was read correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this import to work you need to launch jupyter notebook from the directorythat have the notebook and data\n",
    "data = pd.read_csv('hly3904.csv')\n",
    "print(data.shape)\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that this is a massive dataset with over 200,000 entries that we will cut down on\n",
    "\n",
    "-------\n",
    "We can see that some of the data was read in as objects or strings rather than floats or ints so we need to chnage them to numeric values. Then if there are any blank values interpolate them using a cubic spline so we are not missing data. This should give a good enough approximation considering the size f the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(index=np.arange(0,100000)) #drop the first 100000 hours\n",
    "date_time = pd.to_datetime(data.pop('date'))\n",
    "cols = ['wetb','vappr','rhum','vis']\n",
    "for x in cols:\n",
    "    data[x] = pd.to_numeric(data[x], errors = 'coerce')#data[col].astype(float)\n",
    "    data[x] = data[x].interpolate(method = 'cubic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to remove the categorical data that we do not need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ww','w','ind','ind.1','ind.2','ind.3','ind.4']\n",
    "data = data.drop(cols,axis=1)\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the tutorial we will be combining the windspeed and direction as they are not great predictors on their own, we will then normalise the data around 0, as the last part showed that the model will not train well at all unless everything is on the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = data.pop('wdsp')\n",
    "# Convert to radians.\n",
    "wd_rad = data.pop('wddir')*np.pi / 180\n",
    "# Calculate the wind x and y components.\n",
    "data['Wx'] = wv*np.cos(wd_rad)\n",
    "data['Wy'] = wv*np.sin(wd_rad)\n",
    "\n",
    "timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "data['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "data['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "data['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "data['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "\n",
    "data_mean = data.mean()\n",
    "data_std = data.std()\n",
    "data_df = (data - data_mean)/data_std\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the data again now, we have removed 7 usless predictors and all of the data has been normalised. I want to see If I can remove even more features for better training. I will be looking to see how correlated diffferent columns are with each other, if a column is >90% correlated I will remove it as it is just repeating information that is being provided by another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.corr().abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is hard to pull information from so lets put it into a seaborn heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data_df.corr().abs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this that there is a cluster of highly correlated data in the top left containing temp,wetb,dewpt and vappr. This makes sense as they are all reporting on a type of tempurature. Except vappr that is showing Vapour Pressure. I will remove wetb and dewpt and check again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.drop('wetb', axis=1)\n",
    "data_df = data_df.drop('dewpt',axis=1)\n",
    "sns.heatmap(data_df.corr().abs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This in my opinion looks much better where all features are contributing somthing different. \n",
    "\n",
    "Now lets take a quick look at the data using the plotting code from the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = ['temp', 'rain', 'vis']\n",
    "plot_features = data_df[plot_cols]\n",
    "plot_features.index = date_time\n",
    "_ = plot_features.plot(subplots=True)\n",
    "\n",
    "plot_features = data_df[plot_cols][:480]\n",
    "plot_features.index = date_time[:480]\n",
    "_ = plot_features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train, Test, Validation\n",
    "\n",
    "I have modifed the WindowGenerator function from the tutorial, I have removed the plotting function for a more custom plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = data_df.shape[0]\n",
    "\n",
    "train_df = data_df[0:int(n*0.7)]\n",
    "val_df = data_df[int(n*0.7):int(n*0.9)]\n",
    "test_df = data_df[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                   train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                   label_columns=None):\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "         f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "          data=data,\n",
    "          targets=None,\n",
    "          sequence_length=self.total_window_size,\n",
    "          sequence_stride=1,\n",
    "          shuffle=True,\n",
    "          batch_size=256,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "        # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "        # And cache it for next time\n",
    "        self._example = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Windowed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_STEPS = 48\n",
    "cork_window_temp = WindowGenerator(input_width=72,\n",
    "                               label_width=OUT_STEPS,\n",
    "                               shift=OUT_STEPS, train_df=train_df, val_df=val_df ,test_df=test_df, label_columns = ['temp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model\n",
    "The model below is to predict tempuratue. It is the same as the model that was used in the tutorial section. this time it has a single output layer where the output dimensions are `(None, 48,1)`, To match the windowed data, As I have only asked it to used tempurature in the labels. This should hopfully improve the accuracy. You will see this output shape in the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_features = data_df.columns.size\n",
    "num_features = 15\n",
    "temp_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,kernel_initializer=tf.initializers.zeros()),\n",
    "    #tf.keras.layers.Dense(8, activation = 'selu'),\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, window,MAX_EPOCHS = 3 ,patience=2):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "    \n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "    model.summary()\n",
    "    return history\n",
    "\n",
    "compile_and_fit(temp_lstm_model,cork_window_temp,MAX_EPOCHS=1)\n",
    "\n",
    "for i in range(4):\n",
    "    plot_results(model=temp_lstm_model,window = cork_window_temp, features=['temp'], subplot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see while the mean absolute error is relativly high, I belive this is due to the sheer amount of data as some results are very wrong and some are very close, the model has performed very well in terms of getting the general trends correct. This can be seen espelilly well if you plot several time. Although in my opinion the tempuratue is the easiets to predict of all of the features as it clearly rises and falls for each day cycle. The next model tests the LSTMs ability to predict the tempurature, rainfall and the wind vectors as these are far less tied to the dya night cycle, so I am not expecting as good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = temp_lstm_model.evaluate(cork_window_temp.test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_STEPS = 48\n",
    "cork_window_multi = WindowGenerator(input_width=72,\n",
    "                               label_width=OUT_STEPS,\n",
    "                               shift=OUT_STEPS, train_df=train_df, val_df=val_df ,test_df=test_df, label_columns = ['temp','rain','Wx','Wy'])\n",
    "\n",
    "num_features = 15\n",
    "multi_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,kernel_initializer=tf.initializers.zeros()),\n",
    "    #tf.keras.layers.Dense(8, activation = 'selu'),\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features]),\n",
    "    tf.keras.layers.Dense(4)\n",
    "])\n",
    "compile_and_fit(multi_lstm_model,cork_window_multi)\n",
    "plot_results(model=multi_lstm_model,window = cork_window_multi, features=['temp','rain','Wx','Wy'], subplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output graphs the LSTM appears to have trained very well, and better than I expected. While it is by no means perfect and definatly stuggles the most with predicting the rain the most as it has alot of '0' values. I am impressed that it managed to predict the wint vectors relativly well.\n",
    "\n",
    "I wish I had a more powerful computer so I could run more epochs on a deeper network of LSTMs. I belive there is potential in this to develop quite an accurate model. I would also love to expiriment with trying to predict categorical data using a sliding window as the 'ww' codes that I dropped in the beggining classify the general weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In summary from going through the tutorial and modifying it I have made a few conclusions about training\n",
    "1. For time series data normalising the data is absolutly critical or else convergence will not happen. While this is important for regular models this step is important but not an absolute deal breaker. The model on unnormalised data output stright line almost always and was less than usless \n",
    "\n",
    "2. Making the LSTM layers have more units or adding more LSTM layers is a good way to increase the accuracy and increase convergence. However this comes at the cost of training time. Like CNNs it appers as though one can only really leverage them for all they are worth in terms of depth with more powerful hardware\n",
    "\n",
    "3. From what I could see there appears to be a trade off between giving the model too much historical data vs a small amaount, that is why I landed on giving my Cork model 3 days to predict ahead the next 48\n",
    "\n",
    "4. There did not seem to be much over fitting, I am sure with more complex models this is bound to happen. \n",
    "\n",
    "4. Overall performance was good, espellilly on the tempurature matric. I belive as it simply ocsillates at different frequecies, I think that is why in the correlation heatmap it was correlated to the cos wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "258924b7e9adaccf210136eff2cb525ec43dd0b06a20828be4869db60d742960"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
